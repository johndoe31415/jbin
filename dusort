#!/usr/bin/python3
#	jbin - Joe's miscellaneous scripts, tools and configs
#	dusort: Sortable, cachable "du" clone
#	Copyright (C) 2019-2020 Johannes Bauer
#
#	This file is part of jbin.
#
#	jbin is free software; you can redistribute it and/or modify
#	it under the terms of the GNU General Public License as published by
#	the Free Software Foundation; this program is ONLY licensed under
#	version 3 of the License, later versions are explicitly excluded.
#
#	jbin is distributed in the hope that it will be useful,
#	but WITHOUT ANY WARRANTY; without even the implied warranty of
#	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#	GNU General Public License for more details.
#
#	You should have received a copy of the GNU General Public License
#	along with jbin. If not, see <http://www.gnu.org/licenses/>.
#
#	Johannes Bauer <JohannesBauer@gmx.de>

import sys
import os
import json
import stat
from FriendlyArgumentParser import FriendlyArgumentParser
from FilesizeFormatter import FilesizeFormatter

parser = FriendlyArgumentParser()
parser.add_argument("-c", "--cachefile", metavar = "filename", help = "File to cache results in.")
parser.add_argument("-v", "--verbose", action = "store_true", help = "Be more verbose during the process.")
parser.add_argument("directory", metavar = "directory", nargs = "?", type = str, help = "Directory of which the size is estimated.")
args = parser.parse_args(sys.argv[1:])

fsfmt = FilesizeFormatter()

def get_filesize(filename):
	stat_result = os.stat(filename)
	if stat.S_ISREG(stat_result.st_mode):
		return stat_result.st_blksize * stat_result.st_blocks
	else:
		return 0

if args.cachefile is not None:
	try:
		with open(args.cachefile) as f:
			results = json.load(f)
	except (FileNotFoundError, json.JSONDecodeError):
		results = None
else:
	results = None

if results is None:
	base_dir = args.directory or "."
	if not base_dir.endswith("/"):
		base_dir += "/"

	results = {
		"base_dir": base_dir,
		"total_size": 0,
		"data": { },
	}
	for (dir_name, subdirs, files) in os.walk(base_dir):
		rel_dir = dir_name[len(base_dir):]
		dir_results = {
			"size":	0,
			"errors": 0,
		}
		for filename in files:
			full_filename = dir_name + "/" + filename
			if os.path.isfile(full_filename):
				filesize = get_filesize(full_filename)
				dir_results["size"] += filesize
				results["total_size"] += filesize
		results["data"][rel_dir] = dir_results

if args.cachefile is not None:
	with open(args.cachefile, "w") as f:
		json.dump(results, f)

def upper_directories(relname):
	while True:
		relname = os.path.dirname(relname)
		yield relname
		if relname == "":
			break

# Determine cumulative sizes of everything
for (relname, dir_details) in results["data"].items():
	dir_details["cumulative_size"] = dir_details.get("cumulative_size", 0)
	for upper_dir in upper_directories(relname):
		results["data"][upper_dir]["cumulative_size"] = results["data"][upper_dir].get("cumulative_size", 0) + dir_details["size"]

sorted_data = list(results["data"].items())
sorted_data.sort(key = lambda entry: (entry[1]["size"], entry[0]))
#sorted_data.sort(key = lambda entry: (entry[0], ))
for (relname, dir_details) in sorted_data:
	str_size = fsfmt(dir_details["size"])
	str_cumulative_size = fsfmt(dir_details["cumulative_size"])
	name = results["base_dir"] + relname
	print("%5.1f%% %5.1f%% %10s %10s %s" % (dir_details["size"] / results["total_size"] * 100, dir_details["cumulative_size"] / results["total_size"] * 100, str_size, str_cumulative_size, name))
